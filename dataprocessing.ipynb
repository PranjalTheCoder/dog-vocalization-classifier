{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "435ec0a1",
   "metadata": {},
   "source": [
    "# Husky dataset ‚Äî load train and test\n",
    "This notebook loads `dogvoicedataset/husky` train and test labels and builds full file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a176b076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CSV: ../dogvoicedataset/husky_train_labels.csv\n",
      "Test CSV: ../dogvoicedataset/husky_test_labels.csv\n",
      "Train columns: ['audio_id', 'arousal', 'valence']\n",
      "Test columns : ['audio_id', 'arousal', 'valence']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Paths (notebook is in `model/` so dataset is at ../dogvoicedataset)\n",
    "ROOT = Path('..')\n",
    "DATASET_DIR = ROOT / 'dogvoicedataset'\n",
    "HUSKY_DIR = DATASET_DIR / 'husky'\n",
    "TRAIN_CSV = DATASET_DIR / 'husky_train_labels.csv'\n",
    "TEST_CSV = DATASET_DIR / 'husky_test_labels.csv'\n",
    "\n",
    "print('Training CSV:', TRAIN_CSV)\n",
    "print('Test CSV:', TEST_CSV)\n",
    "\n",
    "# Read CSVs\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "print('Train columns:', list(train_df.columns))\n",
    "print('Test columns :', list(test_df.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bb29a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape, files exist: (600, 5) 0\n",
      "Test  shape, files exist: (100, 5) 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio_id</th>\n",
       "      <th>arousal</th>\n",
       "      <th>valence</th>\n",
       "      <th>full_path</th>\n",
       "      <th>exists</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>husky_train_00000</td>\n",
       "      <td>High</td>\n",
       "      <td>Positive</td>\n",
       "      <td>../dogvoicedataset/husky/train/husky_train_00000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>husky_train_00001</td>\n",
       "      <td>High</td>\n",
       "      <td>Positive</td>\n",
       "      <td>../dogvoicedataset/husky/train/husky_train_00001</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>husky_train_00002</td>\n",
       "      <td>Low</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>../dogvoicedataset/husky/train/husky_train_00002</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>husky_train_00003</td>\n",
       "      <td>Low</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>../dogvoicedataset/husky/train/husky_train_00003</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>husky_train_00004</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Positive</td>\n",
       "      <td>../dogvoicedataset/husky/train/husky_train_00004</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            audio_id arousal   valence  \\\n",
       "0  husky_train_00000    High  Positive   \n",
       "1  husky_train_00001    High  Positive   \n",
       "2  husky_train_00002     Low   Neutral   \n",
       "3  husky_train_00003     Low   Neutral   \n",
       "4  husky_train_00004  Medium  Positive   \n",
       "\n",
       "                                          full_path  exists  \n",
       "0  ../dogvoicedataset/husky/train/husky_train_00000   False  \n",
       "1  ../dogvoicedataset/husky/train/husky_train_00001   False  \n",
       "2  ../dogvoicedataset/husky/train/husky_train_00002   False  \n",
       "3  ../dogvoicedataset/husky/train/husky_train_00003   False  \n",
       "4  ../dogvoicedataset/husky/train/husky_train_00004   False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio_id</th>\n",
       "      <th>arousal</th>\n",
       "      <th>valence</th>\n",
       "      <th>full_path</th>\n",
       "      <th>exists</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>husky_test_00000</td>\n",
       "      <td>Low</td>\n",
       "      <td>Negative</td>\n",
       "      <td>../dogvoicedataset/husky/test/husky_test_00000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>husky_test_00001</td>\n",
       "      <td>Low</td>\n",
       "      <td>Negative</td>\n",
       "      <td>../dogvoicedataset/husky/test/husky_test_00001</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>husky_test_00002</td>\n",
       "      <td>Low</td>\n",
       "      <td>Negative</td>\n",
       "      <td>../dogvoicedataset/husky/test/husky_test_00002</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>husky_test_00003</td>\n",
       "      <td>Low</td>\n",
       "      <td>Negative</td>\n",
       "      <td>../dogvoicedataset/husky/test/husky_test_00003</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>husky_test_00004</td>\n",
       "      <td>Low</td>\n",
       "      <td>Negative</td>\n",
       "      <td>../dogvoicedataset/husky/test/husky_test_00004</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           audio_id arousal   valence  \\\n",
       "0  husky_test_00000     Low  Negative   \n",
       "1  husky_test_00001     Low  Negative   \n",
       "2  husky_test_00002     Low  Negative   \n",
       "3  husky_test_00003     Low  Negative   \n",
       "4  husky_test_00004     Low  Negative   \n",
       "\n",
       "                                        full_path  exists  \n",
       "0  ../dogvoicedataset/husky/test/husky_test_00000   False  \n",
       "1  ../dogvoicedataset/husky/test/husky_test_00001   False  \n",
       "2  ../dogvoicedataset/husky/test/husky_test_00002   False  \n",
       "3  ../dogvoicedataset/husky/test/husky_test_00003   False  \n",
       "4  ../dogvoicedataset/husky/test/husky_test_00004   False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Helper: create a full path column from likely filename/column names\n",
    "def add_full_path(df, split='train'):\n",
    "    df = df.copy()\n",
    "    # Candidate filename columns\n",
    "    candidates = ['filepath', 'file', 'filename', 'wav', 'path']\n",
    "    col = None\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            col = c\n",
    "            break\n",
    "    if col is None:\n",
    "        # fallback: assume first column is the filename\n",
    "        col = df.columns[0]\n",
    "    df['full_path'] = df[col].astype(str).apply(lambda p: str(HUSKY_DIR / split / p))\n",
    "    return df\n",
    "\n",
    "train_df = add_full_path(train_df, 'train')\n",
    "test_df = add_full_path(test_df, 'test')\n",
    "\n",
    "# Check existence\n",
    "train_df['exists'] = train_df['full_path'].apply(lambda p: Path(p).exists())\n",
    "test_df['exists']  = test_df['full_path'].apply(lambda p: Path(p).exists())\n",
    "\n",
    "print('Train shape, files exist:', train_df.shape, train_df['exists'].sum())\n",
    "print('Test  shape, files exist:', test_df.shape, test_df['exists'].sum())\n",
    "\n",
    "display(train_df.head())\n",
    "display(test_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbb0946",
   "metadata": {},
   "source": [
    "Next: if some `exists` values are False, verify the filenames in the CSV match the files under `dogvoicedataset/husky/train` and `dogvoicedataset/husky/test`.\n",
    "You can then proceed to load audio files (e.g., with `librosa.load`) or extract features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13671e6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'distutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlibrosa\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhub\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/dog-vocalization-classifier/.venv/lib/python3.12/site-packages/tensorflow/__init__.py:30\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03mTop-level module of TensorFlow. By convention, we refer to this module as\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[33;03m`tf` instead of `tensorflow`, following the common practice of importing\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m \u001b[33;03mthis file with a file generated from [`api_template.__init__.py`](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/api_template.__init__.py)\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# pylint: disable=g-bad-import-order,protected-access,g-import-not-at-top\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdistutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_distutils\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minspect\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_inspect\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'distutils'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Compatibility shim: Python 3.12 removes distutils from the stdlib.\n",
    "# Some packages (including TensorFlow) still import distutils ‚Äî\n",
    "# map the name to setuptools' vendored copy when missing.\n",
    "try:\n",
    "    import distutils\n",
    "except Exception:\n",
    "    try:\n",
    "        import setuptools._distutils as distutils\n",
    "        sys.modules['distutils'] = distutils\n",
    "    except Exception:\n",
    "        # If setuptools._distutils isn't available, let the import fail later\n",
    "        pass\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018d1120",
   "metadata": {},
   "source": [
    "## Audio Preprocessing Utilities\n",
    "\n",
    "Load and preprocess .wav files to 16kHz mono audio and extract YAMNet embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cea03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_resample_audio(filepath, target_sr=16000):\n",
    "    \"\"\"\n",
    "    Load audio file and resample to target sample rate (mono).\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): Path to .wav file\n",
    "        target_sr (int): Target sample rate (default 16kHz for YAMNet)\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Audio waveform resampled to target_sr\n",
    "    \"\"\"\n",
    "    try:\n",
    "        audio, sr = librosa.load(filepath, sr=None, mono=True)\n",
    "        if sr != target_sr:\n",
    "            audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
    "        return audio\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test: load one audio file\n",
    "sample_audio = load_and_resample_audio(train_df.iloc[0]['full_path'])\n",
    "if sample_audio is not None:\n",
    "    print(f\"Sample audio shape: {sample_audio.shape}, duration: {len(sample_audio) / 16000:.2f}s\")\n",
    "else:\n",
    "    print(\"Failed to load sample audio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0ccce2",
   "metadata": {},
   "source": [
    "## Label Parsing and Encoding\n",
    "\n",
    "Parse label columns and encode them numerically for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db990e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label columns (adjust based on your actual CSV column names)\n",
    "VALENCE_COL = 'valence'  # Expected values: negative, neutral, positive\n",
    "AROUSAL_COL = 'arousal'  # Expected values: low, medium, high\n",
    "\n",
    "# Check actual columns and infer if needed\n",
    "print(\"Train DataFrame columns:\", list(train_df.columns))\n",
    "print(\"Test DataFrame columns:\", list(test_df.columns))\n",
    "\n",
    "# If columns don't exist, show first few rows for manual inspection\n",
    "if VALENCE_COL not in train_df.columns or AROUSAL_COL not in train_df.columns:\n",
    "    print(\"\\n‚ö†Ô∏è  Label columns not found. Showing data for inspection:\")\n",
    "    display(train_df.head())\n",
    "    print(\"\\nüìù Update VALENCE_COL and AROUSAL_COL based on your actual column names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14f8f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "valence_encoder = LabelEncoder()\n",
    "arousal_encoder = LabelEncoder()\n",
    "\n",
    "train_df['valence_encoded'] = valence_encoder.fit_transform(train_df[VALENCE_COL])\n",
    "train_df['arousal_encoded'] = arousal_encoder.fit_transform(train_df[AROUSAL_COL])\n",
    "\n",
    "test_df['valence_encoded'] = valence_encoder.transform(test_df[VALENCE_COL])\n",
    "test_df['arousal_encoded'] = arousal_encoder.transform(test_df[AROUSAL_COL])\n",
    "\n",
    "print(\"Valence classes:\", valence_encoder.classes_)\n",
    "print(\"Arousal classes:\", arousal_encoder.classes_)\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "print(\"\\nTrain labels sample:\")\n",
    "print(train_df[[VALENCE_COL, AROUSAL_COL, 'valence_encoded', 'arousal_encoded']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbcbc7d",
   "metadata": {},
   "source": [
    "## Load YAMNet Model and Extract Embeddings\n",
    "\n",
    "Load the pre-trained YAMNet model and extract embeddings for each audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c9ec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "# Load YAMNet model from TensorFlow Hub\n",
    "# YAMNet is a pre-trained audio event classifier that produces embeddings\n",
    "yamnet_model_url = 'https://tfhub.dev/google/yamnet/1'\n",
    "yamnet = hub.load(yamnet_model_url)\n",
    "\n",
    "print(f\"YAMNet model loaded from {yamnet_model_url}\")\n",
    "print(f\"YAMNet embedding dimension: 1024\")\n",
    "\n",
    "def extract_yamnet_embedding(audio_waveform, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Extract YAMNet embedding from audio waveform.\n",
    "    \n",
    "    Args:\n",
    "        audio_waveform (np.ndarray): Audio samples at sample_rate\n",
    "        sample_rate (int): Sample rate of audio (default 16kHz for YAMNet)\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: YAMNet embedding (1024-dim vector, mean-pooled across time)\n",
    "    \"\"\"\n",
    "    # Ensure waveform is float32 and in the correct shape\n",
    "    audio_tensor = tf.cast(audio_waveform, tf.float32)\n",
    "    \n",
    "    # Get embeddings from YAMNet\n",
    "    _, embeddings, _ = yamnet(audio_tensor)\n",
    "    \n",
    "    # Mean-pool embeddings across time dimension to get a single vector per audio\n",
    "    embedding = tf.reduce_mean(embeddings, axis=0).numpy()\n",
    "    return embedding\n",
    "\n",
    "# Test on one sample\n",
    "test_audio = load_and_resample_audio(train_df.iloc[0]['full_path'])\n",
    "if test_audio is not None:\n",
    "    test_embedding = extract_yamnet_embedding(test_audio)\n",
    "    print(f\"\\nTest embedding shape: {test_embedding.shape}\")\n",
    "    print(f\"Embedding stats - Mean: {test_embedding.mean():.4f}, Std: {test_embedding.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6632d2",
   "metadata": {},
   "source": [
    "## Extract Embeddings for All Data\n",
    "\n",
    "Efficiently extract YAMNet embeddings for all training and test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0cb8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings_for_dataframe(df, max_files=None):\n",
    "    \"\"\"\n",
    "    Extract YAMNet embeddings for all audios in a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataframe with 'full_path' column\n",
    "        max_files (int): Limit number of files to process (for testing)\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Array of embeddings (N, 1024)\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    paths = df['full_path'].values\n",
    "    n_files = min(len(paths), max_files) if max_files else len(paths)\n",
    "    \n",
    "    for i, filepath in enumerate(paths[:n_files]):\n",
    "        if (i + 1) % max(1, n_files // 5) == 0:\n",
    "            print(f\"  Processed {i + 1}/{n_files} files...\")\n",
    "        \n",
    "        audio = load_and_resample_audio(filepath)\n",
    "        if audio is not None:\n",
    "            embedding = extract_yamnet_embedding(audio)\n",
    "            embeddings.append(embedding)\n",
    "        else:\n",
    "            # Use zero embedding as fallback for failed files\n",
    "            embeddings.append(np.zeros(1024))\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Extract embeddings for train and test sets\n",
    "print(\"Extracting embeddings for training set...\")\n",
    "train_embeddings = extract_embeddings_for_dataframe(train_df)\n",
    "print(f\"Train embeddings shape: {train_embeddings.shape}\")\n",
    "\n",
    "print(\"\\nExtracting embeddings for test set...\")\n",
    "test_embeddings = extract_embeddings_for_dataframe(test_df)\n",
    "print(f\"Test embeddings shape: {test_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745629ed",
   "metadata": {},
   "source": [
    "## Build Multi-Task Classifier\n",
    "\n",
    "Create a neural network with shared embeddings baseline and two task-specific output heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05a36de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_multi_task_model(embedding_dim=1024, num_valence_classes=3, num_arousal_classes=3):\n",
    "    \"\"\"\n",
    "    Build a multi-task neural network with shared base and task-specific heads.\n",
    "    \n",
    "    Args:\n",
    "        embedding_dim (int): Dimension of input embeddings (YAMNet = 1024)\n",
    "        num_valence_classes (int): Number of valence classes (default 3)\n",
    "        num_arousal_classes (int): Number of arousal classes (default 3)\n",
    "    \n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled multi-task model with two outputs\n",
    "    \"\"\"\n",
    "    # Input layer for embeddings\n",
    "    embedding_input = tf.keras.Input(shape=(embedding_dim,), name='embedding_input')\n",
    "    \n",
    "    # Shared base (dense layers)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(embedding_input)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Task 1: Valence prediction head\n",
    "    valence_out = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    valence_out = tf.keras.layers.Dropout(0.2)(valence_out)\n",
    "    valence_out = tf.keras.layers.Dense(num_valence_classes, activation='softmax', name='valence_output')(valence_out)\n",
    "    \n",
    "    # Task 2: Arousal prediction head\n",
    "    arousal_out = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    arousal_out = tf.keras.layers.Dropout(0.2)(arousal_out)\n",
    "    arousal_out = tf.keras.layers.Dense(num_arousal_classes, activation='softmax', name='arousal_output')(arousal_out)\n",
    "    \n",
    "    # Build model with two outputs\n",
    "    model = tf.keras.Model(inputs=embedding_input, outputs=[valence_out, arousal_out])\n",
    "    \n",
    "    # Compile with separate losses and metrics for each task\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss={\n",
    "            'valence_output': 'sparse_categorical_crossentropy',\n",
    "            'arousal_output': 'sparse_categorical_crossentropy'\n",
    "        },\n",
    "        loss_weights={'valence_output': 1.0, 'arousal_output': 1.0},\n",
    "        metrics={\n",
    "            'valence_output': ['accuracy'],\n",
    "            'arousal_output': ['accuracy']\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_multi_task_model()\n",
    "print(\"Multi-task model built successfully!\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d4aa39",
   "metadata": {},
   "source": [
    "## Train the Multi-Task Model\n",
    "\n",
    "Train on the training set with validation on a held-out split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059c361e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "X_train = train_embeddings\n",
    "y_valence_train = train_df['valence_encoded'].values\n",
    "y_arousal_train = train_df['arousal_encoded'].values\n",
    "\n",
    "# Split into train/validation\n",
    "X_train_split, X_val_split, y_val_train, y_val_val, y_arom_train, y_arom_val = train_test_split(\n",
    "    X_train, y_valence_train, y_arousal_train,\n",
    "    test_size=0.2, random_state=42, stratify=y_valence_train\n",
    ")\n",
    "\n",
    "print(f\"Train split: {X_train_split.shape}\")\n",
    "print(f\"Validation split: {X_val_split.shape}\")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_split,\n",
    "    {'valence_output': y_val_train, 'arousal_output': y_arom_train},\n",
    "    validation_data=(\n",
    "        X_val_split,\n",
    "        {'valence_output': y_val_val, 'arousal_output': y_arom_val}\n",
    "    ),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5179dc10",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set\n",
    "\n",
    "Measure accuracy and other metrics on the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42bae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluate on test set\n",
    "X_test = test_embeddings\n",
    "y_valence_test = test_df['valence_encoded'].values\n",
    "y_arousal_test = test_df['arousal_encoded'].values\n",
    "\n",
    "# Get predictions\n",
    "valence_preds, arousal_preds = model.predict(X_test)\n",
    "valence_pred_classes = np.argmax(valence_preds, axis=1)\n",
    "arousal_pred_classes = np.argmax(arousal_preds, axis=1)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Valence metrics\n",
    "print(\"\\nüìä VALENCE Metrics:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_valence_test, valence_pred_classes):.4f}\")\n",
    "print(f\"  Precision (macro): {precision_score(y_valence_test, valence_pred_classes, average='macro', zero_division=0):.4f}\")\n",
    "print(f\"  Recall (macro): {recall_score(y_valence_test, valence_pred_classes, average='macro', zero_division=0):.4f}\")\n",
    "print(f\"  F1 (macro): {f1_score(y_valence_test, valence_pred_classes, average='macro', zero_division=0):.4f}\")\n",
    "\n",
    "print(\"\\nValence Classification Report:\")\n",
    "print(classification_report(y_valence_test, valence_pred_classes, \n",
    "                          target_names=valence_encoder.classes_, zero_division=0))\n",
    "\n",
    "# Arousal metrics\n",
    "print(\"\\nüìä AROUSAL Metrics:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_arousal_test, arousal_pred_classes):.4f}\")\n",
    "print(f\"  Precision (macro): {precision_score(y_arousal_test, arousal_pred_classes, average='macro', zero_division=0):.4f}\")\n",
    "print(f\"  Recall (macro): {recall_score(y_arousal_test, arousal_pred_classes, average='macro', zero_division=0):.4f}\")\n",
    "print(f\"  F1 (macro): {f1_score(y_arousal_test, arousal_pred_classes, average='macro', zero_division=0):.4f}\")\n",
    "\n",
    "print(\"\\nArousal Classification Report:\")\n",
    "print(classification_report(y_arousal_test, arousal_pred_classes, \n",
    "                          target_names=arousal_encoder.classes_, zero_division=0))\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation completed!\")\n",
    "\n",
    "# Optional: Plot confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Valence confusion matrix\n",
    "cm_valence = confusion_matrix(y_valence_test, valence_pred_classes)\n",
    "im1 = axes[0].imshow(cm_valence, cmap='Blues')\n",
    "axes[0].set_title('Valence Confusion Matrix')\n",
    "axes[0].set_xticks(range(len(valence_encoder.classes_)))\n",
    "axes[0].set_yticks(range(len(valence_encoder.classes_)))\n",
    "axes[0].set_xticklabels(valence_encoder.classes_)\n",
    "axes[0].set_yticklabels(valence_encoder.classes_)\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "for i in range(len(valence_encoder.classes_)):\n",
    "    for j in range(len(valence_encoder.classes_)):\n",
    "        axes[0].text(j, i, str(cm_valence[i, j]), ha='center', va='center', color='white')\n",
    "\n",
    "# Arousal confusion matrix\n",
    "cm_arousal = confusion_matrix(y_arousal_test, arousal_pred_classes)\n",
    "im2 = axes[1].imshow(cm_arousal, cmap='Greens')\n",
    "axes[1].set_title('Arousal Confusion Matrix')\n",
    "axes[1].set_xticks(range(len(arousal_encoder.classes_)))\n",
    "axes[1].set_yticks(range(len(arousal_encoder.classes_)))\n",
    "axes[1].set_xticklabels(arousal_encoder.classes_)\n",
    "axes[1].set_yticklabels(arousal_encoder.classes_)\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "for i in range(len(arousal_encoder.classes_)):\n",
    "    for j in range(len(arousal_encoder.classes_)):\n",
    "        axes[1].text(j, i, str(cm_arousal[i, j]), ha='center', va='center', color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea105adb",
   "metadata": {},
   "source": [
    "## Inference on New Audio\n",
    "\n",
    "Run inference on a new .wav file to predict valence and arousal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd4bb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_vocalization(audio_path, model, valence_encoder, arousal_encoder):\n",
    "    \"\"\"\n",
    "    Predict valence and arousal for a new audio file.\n",
    "    \n",
    "    Args:\n",
    "        audio_path (str): Path to .wav file\n",
    "        model: Trained multi-task model\n",
    "        valence_encoder: Fitted label encoder for valence classes\n",
    "        arousal_encoder: Fitted label encoder for arousal classes\n",
    "    \n",
    "    Returns:\n",
    "        dict: Prediction results with class labels and confidence scores\n",
    "    \"\"\"\n",
    "    # Load and preprocess audio\n",
    "    audio = load_and_resample_audio(audio_path)\n",
    "    if audio is None:\n",
    "        return None\n",
    "    \n",
    "    # Extract embedding\n",
    "    embedding = extract_yamnet_embedding(audio)\n",
    "    embedding_batch = np.expand_dims(embedding, axis=0)  # Add batch dimension\n",
    "    \n",
    "    # Get predictions\n",
    "    valence_probs, arousal_probs = model.predict(embedding_batch, verbose=0)\n",
    "    \n",
    "    # Decode predictions\n",
    "    valence_class_idx = np.argmax(valence_probs[0])\n",
    "    arousal_class_idx = np.argmax(arousal_probs[0])\n",
    "    \n",
    "    valence_class = valence_encoder.classes_[valence_class_idx]\n",
    "    arousal_class = arousal_encoder.classes_[arousal_class_idx]\n",
    "    \n",
    "    return {\n",
    "        'valence': {\n",
    "            'class': valence_class,\n",
    "            'confidence': float(valence_probs[0][valence_class_idx]),\n",
    "            'all_probs': {valence_encoder.classes_[i]: float(p) \n",
    "                         for i, p in enumerate(valence_probs[0])}\n",
    "        },\n",
    "        'arousal': {\n",
    "            'class': arousal_class,\n",
    "            'confidence': float(arousal_probs[0][arousal_class_idx]),\n",
    "            'all_probs': {arousal_encoder.classes_[i]: float(p) \n",
    "                         for i, p in enumerate(arousal_probs[0])}\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test inference on a sample from test set\n",
    "test_audio_path = test_df.iloc[0]['full_path']\n",
    "print(f\"Testing inference on: {test_audio_path}\")\n",
    "print(f\"True labels - Valence: {test_df.iloc[0][VALENCE_COL]}, Arousal: {test_df.iloc[0][AROUSAL_COL]}\\n\")\n",
    "\n",
    "result = predict_vocalization(test_audio_path, model, valence_encoder, arousal_encoder)\n",
    "\n",
    "if result:\n",
    "    print(\"üîÆ Predictions:\")\n",
    "    print(f\"\\n  Valence: {result['valence']['class']} (confidence: {result['valence']['confidence']:.2%})\")\n",
    "    print(f\"    Probabilities: {result['valence']['all_probs']}\")\n",
    "    \n",
    "    print(f\"\\n  Arousal: {result['arousal']['class']} (confidence: {result['arousal']['confidence']:.2%})\")\n",
    "    print(f\"    Probabilities: {result['arousal']['all_probs']}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to process audio file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b3dc3",
   "metadata": {},
   "source": [
    "## Save and Load the Trained Model\n",
    "\n",
    "Save the trained model and encoders for future inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed21c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save model\n",
    "model_path = 'yamnet_multitask_model.h5'\n",
    "model.save(model_path)\n",
    "print(f\"‚úÖ Model saved to {model_path}\")\n",
    "\n",
    "# Save label encoders\n",
    "encoders_path = 'label_encoders.pkl'\n",
    "with open(encoders_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'valence_encoder': valence_encoder,\n",
    "        'arousal_encoder': arousal_encoder\n",
    "    }, f)\n",
    "print(f\"‚úÖ Label encoders saved to {encoders_path}\")\n",
    "\n",
    "# To load later:\n",
    "# loaded_model = tf.keras.models.load_model(model_path)\n",
    "# with open(encoders_path, 'rb') as f:\n",
    "#     encoders = pickle.load(f)\n",
    "#     valence_encoder = encoders['valence_encoder']\n",
    "#     arousal_encoder = encoders['arousal_encoder']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
